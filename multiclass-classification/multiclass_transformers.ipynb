{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multiclass-transformers.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Aspect category detection with transformers"
      ],
      "metadata": {
        "id": "hIodo8rGQht3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up Google drive and install transformers"
      ],
      "metadata": {
        "id": "s3R-vHght3EA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPV_UxejQUTJ",
        "outputId": "fafe0bf6-457d-45e2-9919-faf8f1193bdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==2.7.0\n",
        "!pip install seqeval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C794btnGQ2l6",
        "outputId": "540bd862-04c1-409c-dab7-a5536cf6ada3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==2.7.0\n",
            "  Downloading transformers-2.7.0-py3-none-any.whl (544 kB)\n",
            "\u001b[K     |████████████████████████████████| 544 kB 8.3 MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "  Downloading boto3-1.20.39-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 70.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==2.7.0) (3.4.2)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 66.2 MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.5.2\n",
            "  Downloading tokenizers-0.5.2-cp37-cp37m-manylinux1_x86_64.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 17.9 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 70.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==2.7.0) (4.62.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.7.0) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.7.0) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==2.7.0) (1.19.5)\n",
            "Collecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting botocore<1.24.0,>=1.23.39\n",
            "  Downloading botocore-1.23.39-py3-none-any.whl (8.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.5 MB 51.5 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 9.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.24.0,>=1.23.39->boto3->transformers==2.7.0) (2.8.2)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.8-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 81.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.24.0,>=1.23.39->boto3->transformers==2.7.0) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.7.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.7.0) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.7.0) (2.10)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 73.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.7.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.7.0) (1.1.0)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, tokenizers, sentencepiece, sacremoses, boto3, transformers\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.20.39 botocore-1.23.39 jmespath-0.10.0 s3transfer-0.5.0 sacremoses-0.0.47 sentencepiece-0.1.96 tokenizers-0.5.2 transformers-2.7.0 urllib3-1.25.11\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.0.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=9b1a19479287bf46502d4cb695b6c844cb719b9281a5d9f1e39bdff6b113acd1\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure model"
      ],
      "metadata": {
        "id": "_G5R83_dubqE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input: train.tsv, dev.tsv and test.tsv + labels file"
      ],
      "metadata": {
        "id": "eO4XgOQMufy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# change model_type and model_name_or_path to change transformer, e.g. roberta-base\n",
        "# https://huggingface.co/models\n",
        "\n",
        "# check that directory is correct for script, label, input and output\n",
        "!python /content/drive/MyDrive/TFM_Rosa-Maria/transformers-training-scripts/run_classification.py --data_dir=/content/drive/MyDrive/TFM_Rosa-Maria/multiclass-classification/data/original  \\\n",
        "    --model_type roberta-base \\\n",
        "    --labels /content/drive/MyDrive/TFM_Rosa-Maria/multiclass-classification/data/labels/classifier-categories-labels.txt \\\n",
        "    --model_name_or_path roberta-base \\\n",
        "    --output_dir=/content/drive/MyDrive/TFM_Rosa-Maria/multiclass-classification/output/original-roberta-three \\\n",
        "    --max_seq_length 128 \\\n",
        "    --num_train_epochs 5 \\\n",
        "    --per_gpu_train_batch_size 32 \\\n",
        "    --learning_rate 5e-5 \\\n",
        "    --save_steps 0 \\\n",
        "\t  --overwrite_cache \\\n",
        "\t  --overwrite_output_dir \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --do_predict \\\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxPSjTEQQ3Er",
        "outputId": "d27b8674-b968-4f57-d4a8-7a7ed4b0eaea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "01/20/2022 12:48:35 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "01/20/2022 12:48:35 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\n",
            "01/20/2022 12:48:35 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 13,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"O\",\n",
            "    \"1\": \"SERVICE#GENERAL\",\n",
            "    \"10\": \"DRINKS#QUALITY\",\n",
            "    \"11\": \"DRINKS#PRICES\",\n",
            "    \"12\": \"AMBIENCE#GENERAL\",\n",
            "    \"2\": \"RESTAURANT#PRICES\",\n",
            "    \"3\": \"RESTAURANT#MISCELLANEOUS\",\n",
            "    \"4\": \"RESTAURANT#GENERAL\",\n",
            "    \"5\": \"LOCATION#GENERAL\",\n",
            "    \"6\": \"FOOD#STYLE_OPTIONS\",\n",
            "    \"7\": \"FOOD#QUALITY\",\n",
            "    \"8\": \"FOOD#PRICES\",\n",
            "    \"9\": \"DRINKS#STYLE_OPTIONS\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"AMBIENCE#GENERAL\": 12,\n",
            "    \"DRINKS#PRICES\": 11,\n",
            "    \"DRINKS#QUALITY\": 10,\n",
            "    \"DRINKS#STYLE_OPTIONS\": 9,\n",
            "    \"FOOD#PRICES\": 8,\n",
            "    \"FOOD#QUALITY\": 7,\n",
            "    \"FOOD#STYLE_OPTIONS\": 6,\n",
            "    \"LOCATION#GENERAL\": 5,\n",
            "    \"O\": 0,\n",
            "    \"RESTAURANT#GENERAL\": 4,\n",
            "    \"RESTAURANT#MISCELLANEOUS\": 3,\n",
            "    \"RESTAURANT#PRICES\": 2,\n",
            "    \"SERVICE#GENERAL\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "01/20/2022 12:48:35 - INFO - __main__ -   Tokenizer arguments: {'do_lower_case': False}\n",
            "01/20/2022 12:48:35 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\n",
            "01/20/2022 12:48:35 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "01/20/2022 12:48:36 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
            "01/20/2022 12:48:36 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "tokenizer object loaded:  <transformers.tokenization_roberta.RobertaTokenizer object at 0x7f16b035da90>\n",
            "01/20/2022 12:48:36 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
            "01/20/2022 12:48:41 - INFO - transformers.modeling_utils -   Weights of RobertaForSequenceClassification not initialized from pretrained model: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "01/20/2022 12:48:41 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "01/20/2022 12:48:46 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='/content/drive/MyDrive/TFM-private/classifier-experiments/original-data/final-data', device=device(type='cuda'), do_eval=True, do_lower_case=False, do_predict=True, do_train=True, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', get_all_preds=False, gradient_accumulation_steps=1, keep_accents=None, labels='/content/drive/MyDrive/TFM-private/transformer-experiments/classifier-categories-labels.txt', learning_rate=5e-05, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='roberta-base', model_type='roberta-base', n_gpu=1, no_cuda=False, num_train_epochs=3.0, output_dir='/content/drive/MyDrive/TFM-private/classifier-experiments/original-data/output-roberta-three', overwrite_cache=True, overwrite_output_dir=True, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=32, save_steps=0, seed=42, server_ip='', server_port='', strip_accents=None, tokenizer_name='', use_fast=None, warmup_steps=0, weight_decay=0.0)\n",
            "01/20/2022 12:48:46 - INFO - __main__ -   Creating features from dataset file at /content/drive/MyDrive/TFM-private/classifier-experiments/original-data/final-data\n",
            "100% 2254/2254 [00:00<00:00, 6166.61it/s]\n",
            "01/20/2022 12:48:47 - INFO - __main__ -   Saving features into cached file /content/drive/MyDrive/TFM-private/classifier-experiments/original-data/final-data/cached_train_roberta-base_128\n",
            "01/20/2022 12:48:47 - INFO - __main__ -   ***** Running training *****\n",
            "01/20/2022 12:48:47 - INFO - __main__ -     Num examples = 2254\n",
            "01/20/2022 12:48:47 - INFO - __main__ -     Num Epochs = 3\n",
            "01/20/2022 12:48:47 - INFO - __main__ -     Instantaneous batch size per GPU = 32\n",
            "01/20/2022 12:48:47 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "01/20/2022 12:48:47 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "01/20/2022 12:48:47 - INFO - __main__ -     Total optimization steps = 213\n",
            "Epoch:   0% 0/3 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/71 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "\n",
            "Iteration:   1% 1/71 [00:00<00:46,  1.50it/s]\u001b[A\n",
            "Iteration:   3% 2/71 [00:01<00:42,  1.62it/s]\u001b[A\n",
            "Iteration:   4% 3/71 [00:01<00:41,  1.65it/s]\u001b[A\n",
            "Iteration:   6% 4/71 [00:02<00:40,  1.67it/s]\u001b[A\n",
            "Iteration:   7% 5/71 [00:03<00:39,  1.67it/s]\u001b[A\n",
            "Iteration:   8% 6/71 [00:03<00:38,  1.68it/s]\u001b[A\n",
            "Iteration:  10% 7/71 [00:04<00:38,  1.68it/s]\u001b[A\n",
            "Iteration:  11% 8/71 [00:04<00:37,  1.69it/s]\u001b[A\n",
            "Iteration:  13% 9/71 [00:05<00:36,  1.68it/s]\u001b[A\n",
            "Iteration:  14% 10/71 [00:05<00:36,  1.68it/s]\u001b[A\n",
            "Iteration:  15% 11/71 [00:06<00:35,  1.68it/s]\u001b[A\n",
            "Iteration:  17% 12/71 [00:07<00:35,  1.68it/s]\u001b[A\n",
            "Iteration:  18% 13/71 [00:07<00:34,  1.68it/s]\u001b[A\n",
            "Iteration:  20% 14/71 [00:08<00:33,  1.68it/s]\u001b[A\n",
            "Iteration:  21% 15/71 [00:08<00:33,  1.67it/s]\u001b[A\n",
            "Iteration:  23% 16/71 [00:09<00:32,  1.68it/s]\u001b[A\n",
            "Iteration:  24% 17/71 [00:10<00:32,  1.67it/s]\u001b[A\n",
            "Iteration:  25% 18/71 [00:10<00:31,  1.67it/s]\u001b[A\n",
            "Iteration:  27% 19/71 [00:11<00:31,  1.67it/s]\u001b[A\n",
            "Iteration:  28% 20/71 [00:11<00:30,  1.67it/s]\u001b[A\n",
            "Iteration:  30% 21/71 [00:12<00:29,  1.67it/s]\u001b[A\n",
            "Iteration:  31% 22/71 [00:13<00:29,  1.66it/s]\u001b[A\n",
            "Iteration:  32% 23/71 [00:13<00:28,  1.67it/s]\u001b[A\n",
            "Iteration:  34% 24/71 [00:14<00:28,  1.67it/s]\u001b[A\n",
            "Iteration:  35% 25/71 [00:14<00:27,  1.67it/s]\u001b[A\n",
            "Iteration:  37% 26/71 [00:15<00:27,  1.66it/s]\u001b[A\n",
            "Iteration:  38% 27/71 [00:16<00:26,  1.67it/s]\u001b[A\n",
            "Iteration:  39% 28/71 [00:16<00:25,  1.67it/s]\u001b[A\n",
            "Iteration:  41% 29/71 [00:17<00:25,  1.67it/s]\u001b[A\n",
            "Iteration:  42% 30/71 [00:17<00:24,  1.66it/s]\u001b[A\n",
            "Iteration:  44% 31/71 [00:18<00:24,  1.66it/s]\u001b[A\n",
            "Iteration:  45% 32/71 [00:19<00:23,  1.66it/s]\u001b[A\n",
            "Iteration:  46% 33/71 [00:19<00:22,  1.66it/s]\u001b[A\n",
            "Iteration:  48% 34/71 [00:20<00:22,  1.66it/s]\u001b[A\n",
            "Iteration:  49% 35/71 [00:20<00:21,  1.66it/s]\u001b[A\n",
            "Iteration:  51% 36/71 [00:21<00:21,  1.65it/s]\u001b[A\n",
            "Iteration:  52% 37/71 [00:22<00:20,  1.65it/s]\u001b[A\n",
            "Iteration:  54% 38/71 [00:22<00:20,  1.65it/s]\u001b[A\n",
            "Iteration:  55% 39/71 [00:23<00:19,  1.65it/s]\u001b[A\n",
            "Iteration:  56% 40/71 [00:24<00:18,  1.65it/s]\u001b[A\n",
            "Iteration:  58% 41/71 [00:24<00:18,  1.66it/s]\u001b[A\n",
            "Iteration:  59% 42/71 [00:25<00:17,  1.66it/s]\u001b[A\n",
            "Iteration:  61% 43/71 [00:25<00:16,  1.65it/s]\u001b[A\n",
            "Iteration:  62% 44/71 [00:26<00:16,  1.65it/s]\u001b[A\n",
            "Iteration:  63% 45/71 [00:27<00:15,  1.65it/s]\u001b[A\n",
            "Iteration:  65% 46/71 [00:27<00:15,  1.65it/s]\u001b[A\n",
            "Iteration:  66% 47/71 [00:28<00:14,  1.64it/s]\u001b[A\n",
            "Iteration:  68% 48/71 [00:28<00:14,  1.64it/s]\u001b[A\n",
            "Iteration:  69% 49/71 [00:29<00:13,  1.64it/s]\u001b[A\n",
            "Iteration:  70% 50/71 [00:30<00:12,  1.64it/s]\u001b[A\n",
            "Iteration:  72% 51/71 [00:30<00:12,  1.64it/s]\u001b[A\n",
            "Iteration:  73% 52/71 [00:31<00:11,  1.64it/s]\u001b[A\n",
            "Iteration:  75% 53/71 [00:31<00:10,  1.64it/s]\u001b[A\n",
            "Iteration:  76% 54/71 [00:32<00:10,  1.64it/s]\u001b[A\n",
            "Iteration:  77% 55/71 [00:33<00:09,  1.64it/s]\u001b[A\n",
            "Iteration:  79% 56/71 [00:33<00:09,  1.64it/s]\u001b[A\n",
            "Iteration:  80% 57/71 [00:34<00:08,  1.64it/s]\u001b[A\n",
            "Iteration:  82% 58/71 [00:34<00:07,  1.64it/s]\u001b[A\n",
            "Iteration:  83% 59/71 [00:35<00:07,  1.64it/s]\u001b[A\n",
            "Iteration:  85% 60/71 [00:36<00:06,  1.64it/s]\u001b[A\n",
            "Iteration:  86% 61/71 [00:36<00:06,  1.64it/s]\u001b[A\n",
            "Iteration:  87% 62/71 [00:37<00:05,  1.64it/s]\u001b[A\n",
            "Iteration:  89% 63/71 [00:38<00:04,  1.65it/s]\u001b[A\n",
            "Iteration:  90% 64/71 [00:38<00:04,  1.64it/s]\u001b[A\n",
            "Iteration:  92% 65/71 [00:39<00:03,  1.64it/s]\u001b[A\n",
            "Iteration:  93% 66/71 [00:39<00:03,  1.63it/s]\u001b[A\n",
            "Iteration:  94% 67/71 [00:40<00:02,  1.63it/s]\u001b[A\n",
            "Iteration:  96% 68/71 [00:41<00:01,  1.63it/s]\u001b[A\n",
            "Iteration:  97% 69/71 [00:41<00:01,  1.63it/s]\u001b[A\n",
            "Iteration:  99% 70/71 [00:42<00:00,  1.63it/s]\u001b[A\n",
            "Iteration: 100% 71/71 [00:42<00:00,  1.67it/s]\n",
            "Epoch:  33% 1/3 [00:42<01:25, 42.64s/it]\n",
            "Iteration:   0% 0/71 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1% 1/71 [00:00<00:42,  1.65it/s]\u001b[A\n",
            "Iteration:   3% 2/71 [00:01<00:42,  1.63it/s]\u001b[A\n",
            "Iteration:   4% 3/71 [00:01<00:41,  1.63it/s]\u001b[A\n",
            "Iteration:   6% 4/71 [00:02<00:41,  1.63it/s]\u001b[A\n",
            "Iteration:   7% 5/71 [00:03<00:40,  1.63it/s]\u001b[A\n",
            "Iteration:   8% 6/71 [00:03<00:40,  1.62it/s]\u001b[A\n",
            "Iteration:  10% 7/71 [00:04<00:39,  1.63it/s]\u001b[A\n",
            "Iteration:  11% 8/71 [00:04<00:38,  1.63it/s]\u001b[A\n",
            "Iteration:  13% 9/71 [00:05<00:37,  1.63it/s]\u001b[A\n",
            "Iteration:  14% 10/71 [00:06<00:37,  1.63it/s]\u001b[A\n",
            "Iteration:  15% 11/71 [00:06<00:36,  1.63it/s]\u001b[A\n",
            "Iteration:  17% 12/71 [00:07<00:36,  1.63it/s]\u001b[A\n",
            "Iteration:  18% 13/71 [00:07<00:35,  1.63it/s]\u001b[A\n",
            "Iteration:  20% 14/71 [00:08<00:35,  1.63it/s]\u001b[A\n",
            "Iteration:  21% 15/71 [00:09<00:34,  1.63it/s]\u001b[A\n",
            "Iteration:  23% 16/71 [00:09<00:33,  1.62it/s]\u001b[A\n",
            "Iteration:  24% 17/71 [00:10<00:33,  1.62it/s]\u001b[A\n",
            "Iteration:  25% 18/71 [00:11<00:32,  1.62it/s]\u001b[A\n",
            "Iteration:  27% 19/71 [00:11<00:32,  1.62it/s]\u001b[A\n",
            "Iteration:  28% 20/71 [00:12<00:31,  1.62it/s]\u001b[A\n",
            "Iteration:  30% 21/71 [00:12<00:31,  1.61it/s]\u001b[A\n",
            "Iteration:  31% 22/71 [00:13<00:30,  1.61it/s]\u001b[A\n",
            "Iteration:  32% 23/71 [00:14<00:29,  1.61it/s]\u001b[A\n",
            "Iteration:  34% 24/71 [00:14<00:29,  1.61it/s]\u001b[A\n",
            "Iteration:  35% 25/71 [00:15<00:28,  1.61it/s]\u001b[A\n",
            "Iteration:  37% 26/71 [00:16<00:27,  1.61it/s]\u001b[A\n",
            "Iteration:  38% 27/71 [00:16<00:27,  1.61it/s]\u001b[A\n",
            "Iteration:  39% 28/71 [00:17<00:26,  1.61it/s]\u001b[A\n",
            "Iteration:  41% 29/71 [00:17<00:26,  1.61it/s]\u001b[A\n",
            "Iteration:  42% 30/71 [00:18<00:25,  1.60it/s]\u001b[A\n",
            "Iteration:  44% 31/71 [00:19<00:24,  1.60it/s]\u001b[A\n",
            "Iteration:  45% 32/71 [00:19<00:24,  1.61it/s]\u001b[A\n",
            "Iteration:  46% 33/71 [00:20<00:23,  1.61it/s]\u001b[A\n",
            "Iteration:  48% 34/71 [00:21<00:22,  1.61it/s]\u001b[A\n",
            "Iteration:  49% 35/71 [00:21<00:22,  1.61it/s]\u001b[A\n",
            "Iteration:  51% 36/71 [00:22<00:21,  1.60it/s]\u001b[A\n",
            "Iteration:  52% 37/71 [00:22<00:21,  1.60it/s]\u001b[A\n",
            "Iteration:  54% 38/71 [00:23<00:20,  1.59it/s]\u001b[A\n",
            "Iteration:  55% 39/71 [00:24<00:20,  1.59it/s]\u001b[A\n",
            "Iteration:  56% 40/71 [00:24<00:19,  1.59it/s]\u001b[A\n",
            "Iteration:  58% 41/71 [00:25<00:18,  1.59it/s]\u001b[A\n",
            "Iteration:  59% 42/71 [00:26<00:18,  1.59it/s]\u001b[A\n",
            "Iteration:  61% 43/71 [00:26<00:17,  1.59it/s]\u001b[A\n",
            "Iteration:  62% 44/71 [00:27<00:16,  1.59it/s]\u001b[A\n",
            "Iteration:  63% 45/71 [00:27<00:16,  1.59it/s]\u001b[A\n",
            "Iteration:  65% 46/71 [00:28<00:15,  1.59it/s]\u001b[A\n",
            "Iteration:  66% 47/71 [00:29<00:15,  1.59it/s]\u001b[A\n",
            "Iteration:  68% 48/71 [00:29<00:14,  1.59it/s]\u001b[A\n",
            "Iteration:  69% 49/71 [00:30<00:13,  1.59it/s]\u001b[A\n",
            "Iteration:  70% 50/71 [00:31<00:13,  1.59it/s]\u001b[A\n",
            "Iteration:  72% 51/71 [00:31<00:12,  1.59it/s]\u001b[A\n",
            "Iteration:  73% 52/71 [00:32<00:11,  1.59it/s]\u001b[A\n",
            "Iteration:  75% 53/71 [00:32<00:11,  1.59it/s]\u001b[A\n",
            "Iteration:  76% 54/71 [00:33<00:10,  1.59it/s]\u001b[A\n",
            "Iteration:  77% 55/71 [00:34<00:10,  1.59it/s]\u001b[A\n",
            "Iteration:  79% 56/71 [00:34<00:09,  1.59it/s]\u001b[A\n",
            "Iteration:  80% 57/71 [00:35<00:08,  1.59it/s]\u001b[A\n",
            "Iteration:  82% 58/71 [00:36<00:08,  1.59it/s]\u001b[A\n",
            "Iteration:  83% 59/71 [00:36<00:07,  1.59it/s]\u001b[A\n",
            "Iteration:  85% 60/71 [00:37<00:06,  1.59it/s]\u001b[A\n",
            "Iteration:  86% 61/71 [00:38<00:06,  1.58it/s]\u001b[A\n",
            "Iteration:  87% 62/71 [00:38<00:05,  1.59it/s]\u001b[A\n",
            "Iteration:  89% 63/71 [00:39<00:05,  1.59it/s]\u001b[A\n",
            "Iteration:  90% 64/71 [00:39<00:04,  1.59it/s]\u001b[A\n",
            "Iteration:  92% 65/71 [00:40<00:03,  1.59it/s]\u001b[A\n",
            "Iteration:  93% 66/71 [00:41<00:03,  1.59it/s]\u001b[A\n",
            "Iteration:  94% 67/71 [00:41<00:02,  1.59it/s]\u001b[A\n",
            "Iteration:  96% 68/71 [00:42<00:01,  1.59it/s]\u001b[A\n",
            "Iteration:  97% 69/71 [00:43<00:01,  1.59it/s]\u001b[A\n",
            "Iteration:  99% 70/71 [00:43<00:00,  1.59it/s]\u001b[A\n",
            "Iteration: 100% 71/71 [00:43<00:00,  1.61it/s]\n",
            "Epoch:  67% 2/3 [01:26<00:43, 43.44s/it]\n",
            "Iteration:   0% 0/71 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1% 1/71 [00:00<00:43,  1.60it/s]\u001b[A\n",
            "Iteration:   3% 2/71 [00:01<00:43,  1.59it/s]\u001b[A\n",
            "Iteration:   4% 3/71 [00:01<00:42,  1.58it/s]\u001b[A\n",
            "Iteration:   6% 4/71 [00:02<00:42,  1.59it/s]\u001b[A\n",
            "Iteration:   7% 5/71 [00:03<00:41,  1.59it/s]\u001b[A\n",
            "Iteration:   8% 6/71 [00:03<00:40,  1.59it/s]\u001b[A\n",
            "Iteration:  10% 7/71 [00:04<00:40,  1.59it/s]\u001b[A\n",
            "Iteration:  11% 8/71 [00:05<00:39,  1.59it/s]\u001b[A\n",
            "Iteration:  13% 9/71 [00:05<00:39,  1.59it/s]\u001b[A\n",
            "Iteration:  14% 10/71 [00:06<00:38,  1.59it/s]\u001b[A\n",
            "Iteration:  15% 11/71 [00:06<00:37,  1.59it/s]\u001b[A\n",
            "Iteration:  17% 12/71 [00:07<00:37,  1.59it/s]\u001b[A\n",
            "Iteration:  18% 13/71 [00:08<00:36,  1.59it/s]\u001b[A\n",
            "Iteration:  20% 14/71 [00:08<00:35,  1.59it/s]\u001b[A\n",
            "Iteration:  21% 15/71 [00:09<00:35,  1.59it/s]\u001b[A\n",
            "Iteration:  23% 16/71 [00:10<00:34,  1.59it/s]\u001b[A\n",
            "Iteration:  24% 17/71 [00:10<00:34,  1.58it/s]\u001b[A\n",
            "Iteration:  25% 18/71 [00:11<00:33,  1.58it/s]\u001b[A\n",
            "Iteration:  27% 19/71 [00:11<00:32,  1.58it/s]\u001b[A\n",
            "Iteration:  28% 20/71 [00:12<00:32,  1.58it/s]\u001b[A\n",
            "Iteration:  30% 21/71 [00:13<00:31,  1.58it/s]\u001b[A\n",
            "Iteration:  31% 22/71 [00:13<00:30,  1.58it/s]\u001b[A\n",
            "Iteration:  32% 23/71 [00:14<00:30,  1.58it/s]\u001b[A\n",
            "Iteration:  34% 24/71 [00:15<00:29,  1.57it/s]\u001b[A\n",
            "Iteration:  35% 25/71 [00:15<00:29,  1.57it/s]\u001b[A\n",
            "Iteration:  37% 26/71 [00:16<00:28,  1.57it/s]\u001b[A\n",
            "Iteration:  38% 27/71 [00:17<00:28,  1.57it/s]\u001b[A\n",
            "Iteration:  39% 28/71 [00:17<00:27,  1.57it/s]\u001b[A\n",
            "Iteration:  41% 29/71 [00:18<00:26,  1.57it/s]\u001b[A\n",
            "Iteration:  42% 30/71 [00:18<00:26,  1.57it/s]\u001b[A\n",
            "Iteration:  44% 31/71 [00:19<00:25,  1.57it/s]\u001b[A\n",
            "Iteration:  45% 32/71 [00:20<00:24,  1.57it/s]\u001b[A\n",
            "Iteration:  46% 33/71 [00:20<00:24,  1.57it/s]\u001b[A\n",
            "Iteration:  48% 34/71 [00:21<00:23,  1.57it/s]\u001b[A\n",
            "Iteration:  49% 35/71 [00:22<00:23,  1.56it/s]\u001b[A\n",
            "Iteration:  51% 36/71 [00:22<00:22,  1.56it/s]\u001b[A\n",
            "Iteration:  52% 37/71 [00:23<00:21,  1.56it/s]\u001b[A\n",
            "Iteration:  54% 38/71 [00:24<00:21,  1.56it/s]\u001b[A\n",
            "Iteration:  55% 39/71 [00:24<00:20,  1.56it/s]\u001b[A\n",
            "Iteration:  56% 40/71 [00:25<00:19,  1.56it/s]\u001b[A\n",
            "Iteration:  58% 41/71 [00:26<00:19,  1.55it/s]\u001b[A\n",
            "Iteration:  59% 42/71 [00:26<00:18,  1.55it/s]\u001b[A\n",
            "Iteration:  61% 43/71 [00:27<00:17,  1.56it/s]\u001b[A\n",
            "Iteration:  62% 44/71 [00:27<00:17,  1.56it/s]\u001b[A\n",
            "Iteration:  63% 45/71 [00:28<00:16,  1.55it/s]\u001b[A\n",
            "Iteration:  65% 46/71 [00:29<00:16,  1.56it/s]\u001b[A\n",
            "Iteration:  66% 47/71 [00:29<00:15,  1.56it/s]\u001b[A\n",
            "Iteration:  68% 48/71 [00:30<00:14,  1.55it/s]\u001b[A\n",
            "Iteration:  69% 49/71 [00:31<00:14,  1.55it/s]\u001b[A\n",
            "Iteration:  70% 50/71 [00:31<00:13,  1.55it/s]\u001b[A\n",
            "Iteration:  72% 51/71 [00:32<00:12,  1.55it/s]\u001b[A\n",
            "Iteration:  73% 52/71 [00:33<00:12,  1.55it/s]\u001b[A\n",
            "Iteration:  75% 53/71 [00:33<00:11,  1.56it/s]\u001b[A\n",
            "Iteration:  76% 54/71 [00:34<00:10,  1.56it/s]\u001b[A\n",
            "Iteration:  77% 55/71 [00:35<00:10,  1.56it/s]\u001b[A\n",
            "Iteration:  79% 56/71 [00:35<00:09,  1.56it/s]\u001b[A\n",
            "Iteration:  80% 57/71 [00:36<00:09,  1.55it/s]\u001b[A\n",
            "Iteration:  82% 58/71 [00:36<00:08,  1.55it/s]\u001b[A\n",
            "Iteration:  83% 59/71 [00:37<00:07,  1.55it/s]\u001b[A\n",
            "Iteration:  85% 60/71 [00:38<00:07,  1.55it/s]\u001b[A\n",
            "Iteration:  86% 61/71 [00:38<00:06,  1.55it/s]\u001b[A\n",
            "Iteration:  87% 62/71 [00:39<00:05,  1.55it/s]\u001b[A\n",
            "Iteration:  89% 63/71 [00:40<00:05,  1.54it/s]\u001b[A\n",
            "Iteration:  90% 64/71 [00:40<00:04,  1.54it/s]\u001b[A\n",
            "Iteration:  92% 65/71 [00:41<00:03,  1.54it/s]\u001b[A\n",
            "Iteration:  93% 66/71 [00:42<00:03,  1.54it/s]\u001b[A\n",
            "Iteration:  94% 67/71 [00:42<00:02,  1.54it/s]\u001b[A\n",
            "Iteration:  96% 68/71 [00:43<00:01,  1.54it/s]\u001b[A\n",
            "Iteration:  97% 69/71 [00:44<00:01,  1.54it/s]\u001b[A\n",
            "Iteration:  99% 70/71 [00:44<00:00,  1.54it/s]\u001b[A\n",
            "Iteration: 100% 71/71 [00:45<00:00,  1.58it/s]\n",
            "Epoch: 100% 3/3 [02:11<00:00, 43.90s/it]\n",
            "01/20/2022 12:50:59 - INFO - __main__ -    global_step = 213, average loss = 1.3536123138078502\n",
            "01/20/2022 12:50:59 - INFO - __main__ -   Saving model checkpoint to /content/drive/MyDrive/TFM-private/classifier-experiments/original-data/output-roberta-three\n",
            "01/20/2022 12:50:59 - INFO - transformers.configuration_utils -   Configuration saved in /content/drive/MyDrive/TFM-private/classifier-experiments/original-data/output-roberta-three/config.json\n",
            "01/20/2022 12:51:01 - INFO - transformers.modeling_utils -   Model weights saved in /content/drive/MyDrive/TFM-private/classifier-experiments/original-data/output-roberta-three/pytorch_model.bin\n",
            "01/20/2022 12:51:01 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/TFM-private/classifier-experiments/original-data/output-roberta-three/config.json\n",
            "01/20/2022 12:51:01 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 13,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"O\",\n",
            "    \"1\": \"SERVICE#GENERAL\",\n",
            "    \"2\": \"RESTAURANT#PRICES\",\n",
            "    \"3\": \"RESTAURANT#MISCELLANEOUS\",\n",
            "    \"4\": \"RESTAURANT#GENERAL\",\n",
            "    \"5\": \"LOCATION#GENERAL\",\n",
            "    \"6\": \"FOOD#STYLE_OPTIONS\",\n",
            "    \"7\": \"FOOD#QUALITY\",\n",
            "    \"8\": \"FOOD#PRICES\",\n",
            "    \"9\": \"DRINKS#STYLE_OPTIONS\",\n",
            "    \"10\": \"DRINKS#QUALITY\",\n",
            "    \"11\": \"DRINKS#PRICES\",\n",
            "    \"12\": \"AMBIENCE#GENERAL\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"AMBIENCE#GENERAL\": 12,\n",
            "    \"DRINKS#PRICES\": 11,\n",
            "    \"DRINKS#QUALITY\": 10,\n",
            "    \"DRINKS#STYLE_OPTIONS\": 9,\n",
            "    \"FOOD#PRICES\": 8,\n",
            "    \"FOOD#QUALITY\": 7,\n",
            "    \"FOOD#STYLE_OPTIONS\": 6,\n",
            "    \"LOCATION#GENERAL\": 5,\n",
            "    \"O\": 0,\n",
            "    \"RESTAURANT#GENERAL\": 4,\n",
            "    \"RESTAURANT#MISCELLANEOUS\": 3,\n",
            "    \"RESTAURANT#PRICES\": 2,\n",
            "    \"SERVICE#GENERAL\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "01/20/2022 12:51:01 - INFO - transformers.tokenization_utils -   Model name '/content/drive/MyDrive/TFM-private/classifier-experiments/original-data/output-roberta-three' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming '/content/drive/MyDrive/TFM-private/classifier-experiments/original-data/output-roberta-three' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "01/20/2022 12:51:01 - INFO - transformers.tokenization_utils -   Didn't find file /content/drive/MyDrive/TFM-private/classifier-experiments/original-data/output-roberta-three/added_tokens.json. We won't load it.\n",
            "01/20/2022 12:51:02 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/TFM-private/classifier-experiments/original-data/output-roberta-three/vocab.json\n",
            "01/20/2022 12:51:02 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/TFM-private/classifier-experiments/original-data/output-roberta-three/merges.txt\n",
            "01/20/2022 12:51:02 - INFO - transformers.tokenization_utils -   loading file None\n",
            "01/20/2022 12:51:02 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/TFM-private/classifier-experiments/original-data/output-roberta-three/special_tokens_map.json\n",
            "01/20/2022 12:51:02 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/TFM-private/classifier-experiments/original-data/output-roberta-three/tokenizer_config.json\n",
            "01/20/2022 12:51:02 - INFO - __main__ -   Evaluate the following checkpoints: ['/content/drive/MyDrive/TFM-private/classifier-experiments/original-data/output-roberta-three']\n",
            "01/20/2022 12:51:02 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/TFM-private/classifier-experiments/original-data/output-roberta-three/config.json\n",
            "01/20/2022 12:51:02 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 13,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"O\",\n",
            "    \"1\": \"SERVICE#GENERAL\",\n",
            "    \"2\": \"RESTAURANT#PRICES\",\n",
            "    \"3\": \"RESTAURANT#MISCELLANEOUS\",\n",
            "    \"4\": \"RESTAURANT#GENERAL\",\n",
            "    \"5\": \"LOCATION#GENERAL\",\n",
            "    \"6\": \"FOOD#STYLE_OPTIONS\",\n",
            "    \"7\": \"FOOD#QUALITY\",\n",
            "    \"8\": \"FOOD#PRICES\",\n",
            "    \"9\": \"DRINKS#STYLE_OPTIONS\",\n",
            "    \"10\": \"DRINKS#QUALITY\",\n",
            "    \"11\": \"DRINKS#PRICES\",\n",
            "    \"12\": \"AMBIENCE#GENERAL\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"AMBIENCE#GENERAL\": 12,\n",
            "    \"DRINKS#PRICES\": 11,\n",
            "    \"DRINKS#QUALITY\": 10,\n",
            "    \"DRINKS#STYLE_OPTIONS\": 9,\n",
            "    \"FOOD#PRICES\": 8,\n",
            "    \"FOOD#QUALITY\": 7,\n",
            "    \"FOOD#STYLE_OPTIONS\": 6,\n",
            "    \"LOCATION#GENERAL\": 5,\n",
            "    \"O\": 0,\n",
            "    \"RESTAURANT#GENERAL\": 4,\n",
            "    \"RESTAURANT#MISCELLANEOUS\": 3,\n",
            "    \"RESTAURANT#PRICES\": 2,\n",
            "    \"SERVICE#GENERAL\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "01/20/2022 12:51:02 - INFO - transformers.modeling_utils -   loading weights file /content/drive/MyDrive/TFM-private/classifier-experiments/original-data/output-roberta-three/pytorch_model.bin\n",
            "01/20/2022 12:51:05 - INFO - __main__ -   Creating features from dataset file at /content/drive/MyDrive/TFM-private/classifier-experiments/original-data/final-data\n",
            "100% 2254/2254 [00:00<00:00, 6024.07it/s]\n",
            "01/20/2022 12:51:06 - INFO - __main__ -   Saving features into cached file /content/drive/MyDrive/TFM-private/classifier-experiments/original-data/final-data/cached_dev_roberta-base_128\n",
            "01/20/2022 12:51:07 - INFO - __main__ -   ***** Running evaluation  on  dev set*****\n",
            "01/20/2022 12:51:07 - INFO - __main__ -     Num examples = 2254\n",
            "01/20/2022 12:51:07 - INFO - __main__ -     Batch size = 8\n",
            "Evaluating: 100% 282/282 [00:17<00:00, 16.02it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "01/20/2022 12:51:24 - INFO - __main__ -   ***** Eval results  on dev set*****\n",
            "01/20/2022 12:51:24 - INFO - __main__ -     f1 = 0.6956521739130435\n",
            "01/20/2022 12:51:24 - INFO - __main__ -     loss = 0.8286678044931263\n",
            "01/20/2022 12:51:24 - INFO - __main__ -     precision = 0.6956521739130435\n",
            "01/20/2022 12:51:24 - INFO - __main__ -     recall = 0.6956521739130435\n",
            "01/20/2022 12:51:24 - INFO - __main__ -     report =\n",
            "                           precision    recall  f1-score   support\n",
            "\n",
            "        AMBIENCE#GENERAL     0.5876    0.7124    0.6440       226\n",
            "           DRINKS#PRICES     0.0000    0.0000    0.0000        20\n",
            "          DRINKS#QUALITY     0.4051    0.6957    0.5120        46\n",
            "    DRINKS#STYLE_OPTIONS     0.5000    0.1000    0.1667        30\n",
            "             FOOD#PRICES     0.4583    0.5366    0.4944        82\n",
            "            FOOD#QUALITY     0.6639    0.8267    0.7364       681\n",
            "      FOOD#STYLE_OPTIONS     0.6897    0.1562    0.2548       128\n",
            "        LOCATION#GENERAL     0.0000    0.0000    0.0000        28\n",
            "      RESTAURANT#GENERAL     0.8765    0.8513    0.8637       417\n",
            "RESTAURANT#MISCELLANEOUS     0.8600    0.4433    0.5850        97\n",
            "       RESTAURANT#PRICES     0.5303    0.4375    0.4795        80\n",
            "         SERVICE#GENERAL     0.7781    0.7446    0.7610       419\n",
            "\n",
            "                accuracy                         0.6957      2254\n",
            "               macro avg     0.5291    0.4587    0.4581      2254\n",
            "            weighted avg     0.6929    0.6957    0.6756      2254\n",
            "\n",
            "01/20/2022 12:51:24 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/TFM-private/classifier-experiments/original-data/output-roberta-three/config.json\n",
            "01/20/2022 12:51:24 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 13,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"O\",\n",
            "    \"1\": \"SERVICE#GENERAL\",\n",
            "    \"2\": \"RESTAURANT#PRICES\",\n",
            "    \"3\": \"RESTAURANT#MISCELLANEOUS\",\n",
            "    \"4\": \"RESTAURANT#GENERAL\",\n",
            "    \"5\": \"LOCATION#GENERAL\",\n",
            "    \"6\": \"FOOD#STYLE_OPTIONS\",\n",
            "    \"7\": \"FOOD#QUALITY\",\n",
            "    \"8\": \"FOOD#PRICES\",\n",
            "    \"9\": \"DRINKS#STYLE_OPTIONS\",\n",
            "    \"10\": \"DRINKS#QUALITY\",\n",
            "    \"11\": \"DRINKS#PRICES\",\n",
            "    \"12\": \"AMBIENCE#GENERAL\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"AMBIENCE#GENERAL\": 12,\n",
            "    \"DRINKS#PRICES\": 11,\n",
            "    \"DRINKS#QUALITY\": 10,\n",
            "    \"DRINKS#STYLE_OPTIONS\": 9,\n",
            "    \"FOOD#PRICES\": 8,\n",
            "    \"FOOD#QUALITY\": 7,\n",
            "    \"FOOD#STYLE_OPTIONS\": 6,\n",
            "    \"LOCATION#GENERAL\": 5,\n",
            "    \"O\": 0,\n",
            "    \"RESTAURANT#GENERAL\": 4,\n",
            "    \"RESTAURANT#MISCELLANEOUS\": 3,\n",
            "    \"RESTAURANT#PRICES\": 2,\n",
            "    \"SERVICE#GENERAL\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "01/20/2022 12:51:24 - INFO - transformers.tokenization_utils -   Model name '/content/drive/MyDrive/TFM-private/classifier-experiments/original-data/output-roberta-three' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming '/content/drive/MyDrive/TFM-private/classifier-experiments/original-data/output-roberta-three' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "01/20/2022 12:51:24 - INFO - transformers.tokenization_utils -   Didn't find file /content/drive/MyDrive/TFM-private/classifier-experiments/original-data/output-roberta-three/added_tokens.json. We won't load it.\n",
            "01/20/2022 12:51:24 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/TFM-private/classifier-experiments/original-data/output-roberta-three/vocab.json\n",
            "01/20/2022 12:51:24 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/TFM-private/classifier-experiments/original-data/output-roberta-three/merges.txt\n",
            "01/20/2022 12:51:24 - INFO - transformers.tokenization_utils -   loading file None\n",
            "01/20/2022 12:51:24 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/TFM-private/classifier-experiments/original-data/output-roberta-three/special_tokens_map.json\n",
            "01/20/2022 12:51:24 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/TFM-private/classifier-experiments/original-data/output-roberta-three/tokenizer_config.json\n",
            "01/20/2022 12:51:24 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/TFM-private/classifier-experiments/original-data/output-roberta-three/config.json\n",
            "01/20/2022 12:51:24 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 13,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"O\",\n",
            "    \"1\": \"SERVICE#GENERAL\",\n",
            "    \"2\": \"RESTAURANT#PRICES\",\n",
            "    \"3\": \"RESTAURANT#MISCELLANEOUS\",\n",
            "    \"4\": \"RESTAURANT#GENERAL\",\n",
            "    \"5\": \"LOCATION#GENERAL\",\n",
            "    \"6\": \"FOOD#STYLE_OPTIONS\",\n",
            "    \"7\": \"FOOD#QUALITY\",\n",
            "    \"8\": \"FOOD#PRICES\",\n",
            "    \"9\": \"DRINKS#STYLE_OPTIONS\",\n",
            "    \"10\": \"DRINKS#QUALITY\",\n",
            "    \"11\": \"DRINKS#PRICES\",\n",
            "    \"12\": \"AMBIENCE#GENERAL\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"AMBIENCE#GENERAL\": 12,\n",
            "    \"DRINKS#PRICES\": 11,\n",
            "    \"DRINKS#QUALITY\": 10,\n",
            "    \"DRINKS#STYLE_OPTIONS\": 9,\n",
            "    \"FOOD#PRICES\": 8,\n",
            "    \"FOOD#QUALITY\": 7,\n",
            "    \"FOOD#STYLE_OPTIONS\": 6,\n",
            "    \"LOCATION#GENERAL\": 5,\n",
            "    \"O\": 0,\n",
            "    \"RESTAURANT#GENERAL\": 4,\n",
            "    \"RESTAURANT#MISCELLANEOUS\": 3,\n",
            "    \"RESTAURANT#PRICES\": 2,\n",
            "    \"SERVICE#GENERAL\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "01/20/2022 12:51:24 - INFO - transformers.modeling_utils -   loading weights file /content/drive/MyDrive/TFM-private/classifier-experiments/original-data/output-roberta-three/pytorch_model.bin\n",
            "01/20/2022 12:51:28 - INFO - __main__ -   Creating features from dataset file at /content/drive/MyDrive/TFM-private/classifier-experiments/original-data/final-data\n",
            "100% 742/742 [00:00<00:00, 2560.75it/s]\n",
            "01/20/2022 12:51:28 - INFO - __main__ -   Saving features into cached file /content/drive/MyDrive/TFM-private/classifier-experiments/original-data/final-data/cached_test_roberta-base_128\n",
            "01/20/2022 12:51:29 - INFO - __main__ -   ***** Running evaluation  on  test set*****\n",
            "01/20/2022 12:51:29 - INFO - __main__ -     Num examples = 742\n",
            "01/20/2022 12:51:29 - INFO - __main__ -     Batch size = 8\n",
            "Evaluating: 100% 93/93 [00:05<00:00, 15.96it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "01/20/2022 12:51:35 - INFO - __main__ -   ***** Eval results  on test set*****\n",
            "01/20/2022 12:51:35 - INFO - __main__ -     f1 = 0.6563342318059299\n",
            "01/20/2022 12:51:35 - INFO - __main__ -     loss = 0.9858714507151676\n",
            "01/20/2022 12:51:35 - INFO - __main__ -     precision = 0.6563342318059299\n",
            "01/20/2022 12:51:35 - INFO - __main__ -     recall = 0.6563342318059299\n",
            "01/20/2022 12:51:35 - INFO - __main__ -     report =\n",
            "                           precision    recall  f1-score   support\n",
            "\n",
            "        AMBIENCE#GENERAL     0.5185    0.7368    0.6087        57\n",
            "           DRINKS#PRICES     0.0000    0.0000    0.0000         3\n",
            "          DRINKS#QUALITY     0.4000    0.3810    0.3902        21\n",
            "    DRINKS#STYLE_OPTIONS     0.0000    0.0000    0.0000        12\n",
            "             FOOD#PRICES     0.2800    0.3182    0.2979        22\n",
            "            FOOD#QUALITY     0.6477    0.8578    0.7380       225\n",
            "      FOOD#STYLE_OPTIONS     0.5714    0.0833    0.1455        48\n",
            "        LOCATION#GENERAL     0.0000    0.0000    0.0000        13\n",
            "      RESTAURANT#GENERAL     0.8148    0.7746    0.7942       142\n",
            "RESTAURANT#MISCELLANEOUS     0.5000    0.1515    0.2326        33\n",
            "       RESTAURANT#PRICES     0.6429    0.4286    0.5143        21\n",
            "         SERVICE#GENERAL     0.7171    0.7517    0.7340       145\n",
            "\n",
            "                accuracy                         0.6563       742\n",
            "               macro avg     0.4244    0.3736    0.3713       742\n",
            "            weighted avg     0.6293    0.6563    0.6202       742\n",
            "\n"
          ]
        }
      ]
    }
  ]
}